{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm_garch_bayesian_v3_f",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKd5HTrSygWv"
      },
      "source": [
        "pip install ax-platform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gg_HoPtl2Sc"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import datetime\n",
        "import math\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable \n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from ax.plot.contour import plot_contour\n",
        "from ax.plot.trace import optimization_trace_single_method\n",
        "from ax.service.managed_loop import optimize\n",
        "from ax.utils.notebook.plotting import render\n",
        "from ax.core.simple_experiment import SimpleExperiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm_Kh5OVmGh7"
      },
      "source": [
        "#mount to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asu0GxqARTfH"
      },
      "source": [
        "cd /content/drive/MyDrive/2021/paper/garch_data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2QbHQ1vRTTP"
      },
      "source": [
        "# reading the data\n",
        "start_time = time.time()\n",
        "VIX = pd.read_excel('VIX_data.xlsx', converters= {'Date': pd.to_datetime})\n",
        "garch = pd.read_excel('Total_stock_garch_vol_data_v3.xlsx', converters= {'Date': pd.to_datetime})\n",
        "egarch = pd.read_excel('Total_stock_egarch_vol_data_v3.xlsx', converters= {'Date': pd.to_datetime})\n",
        "print(\"---{}s seconds(read data)---\".format(time.time()-start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1VEjEUdnU7R"
      },
      "source": [
        "def date2index(df):\n",
        "  df.index = df['Date']\n",
        "  return df.drop(columns='Date')\n",
        "\n",
        "def reshaping(nmp):\n",
        "  (x,y)=nmp.shape\n",
        "  nmp = nmp.reshape(x,1,y)\n",
        "  return nmp\n",
        "\n",
        "\n",
        "def make_train_test(df, VIX, train_size):\n",
        "  mm = MinMaxScaler()\n",
        "  ss = StandardScaler()\n",
        "\n",
        "  df_X_ss = ss.fit_transform(df)\n",
        "  df_y_mm = mm.fit_transform(VIX)\n",
        "\n",
        "  X_train = df_X_ss[:train_size, :]\n",
        "  X_test = df_X_ss[train_size:, :]\n",
        "  \n",
        "  y_train = df_y_mm[:train_size, :]\n",
        "  y_test = df_y_mm[train_size:, :]\n",
        "\n",
        "  X_train = reshaping(X_train)\n",
        "  X_test = reshaping(X_test)\n",
        "\n",
        "  print(\"Training Shape\", X_train.shape, y_train.shape)\n",
        "  print(\"Testing Shape\", X_test.shape, y_test.shape) \n",
        "\n",
        "  return (X_train, y_train, X_test, y_test)\n",
        "\n",
        "\n",
        "def make_train_test_tensor(df, VIX, train_size):\n",
        "  mm = MinMaxScaler()\n",
        "  ss = StandardScaler()\n",
        "\n",
        "  df_X_ss = ss.fit_transform(df)\n",
        "  df_y_mm = mm.fit_transform(VIX)\n",
        "\n",
        "  X_train = df_X_ss[:train_size, :]\n",
        "  X_test = df_X_ss[train_size:, :]\n",
        "  \n",
        "  y_train = df_y_mm[:train_size, :]\n",
        "  y_test = df_y_mm[train_size:, :]\n",
        "\n",
        "  X_train_tensors = Variable(torch.Tensor(X_train))\n",
        "  X_test_tensors = Variable(torch.Tensor(X_test))\n",
        "\n",
        "  y_train_tensors = Variable(torch.Tensor(y_train))\n",
        "  y_test_tensors = Variable(torch.Tensor(y_test))\n",
        "\n",
        "  X_train_tensors_final = torch.reshape(X_train_tensors,   (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))\n",
        "  X_test_tensors_final = torch.reshape(X_test_tensors,  (X_test_tensors.shape[0], 1, X_test_tensors.shape[1])) \n",
        "\n",
        "  print(\"Training tensor Shape\", X_train_tensors_final.shape, y_train_tensors.shape)\n",
        "  print(\"Testing tensor Shape\", X_test_tensors_final.shape, y_test_tensors.shape) \n",
        "\n",
        "  return (X_train_tensors_final,X_test_tensors_final,y_train_tensors,y_test_tensors)\n",
        "  \n",
        "\n",
        "def dataloader2tensor(dataloader):\n",
        "  i = 0\n",
        "  for inputs, labels in train_loader:\n",
        "    i+=1\n",
        "    if i == 1:\n",
        "      x = inputs\n",
        "      y = labels\n",
        "    else:\n",
        "      x = torch.cat((x,inputs),0)\n",
        "      y = torch.cat((y,labels),0)\n",
        "  return x, y\n",
        "\n",
        "\n",
        "def visualize(df, VIX, model, train_size):\n",
        "  mm = MinMaxScaler()\n",
        "  ss = StandardScaler()\n",
        "\n",
        "  df_X_ss = ss.fit_transform(df)\n",
        "  df_y_mm = mm.fit_transform(VIX)\n",
        "\n",
        "  #converting to Tensors\n",
        "  df_X_ss = Variable(torch.Tensor(df_X_ss)) \n",
        "  df_y_mm = Variable(torch.Tensor(df_y_mm))\n",
        "\n",
        "  #reshaping the dataset\n",
        "  df_X_ss = torch.reshape(df_X_ss, (df_X_ss.shape[0], 1, df_X_ss.shape[1]))\n",
        "  train_predict = model(df_X_ss.to(device))#forward pass\n",
        "  data_predict = train_predict.data.detach().cpu().numpy() #numpy conversion\n",
        "  dataY_plot = df_y_mm.data.numpy()\n",
        "\n",
        "  data_predict = mm.inverse_transform(data_predict) #reverse transformation\n",
        "  dataY_plot = mm.inverse_transform(dataY_plot)\n",
        "  plt.figure(figsize=(10,6)) #plotting\n",
        "  plt.axvline(x=train_size, c='r', linestyle='--') #size of the training set\n",
        "\n",
        "  plt.plot(dataY_plot, label='Actual Data') #actual plot\n",
        "  plt.plot(data_predict, label='Predicted Data') #predicted plot\n",
        "  plt.title('Time-Series Prediction')\n",
        "  plt.legend()\n",
        "  plt.show() \n",
        "  return dataY_plot, data_predict\n",
        "\n",
        "\n",
        "def errors(GT, DP, train_size):\n",
        "  GT = GT[train_size:]\n",
        "  DP = DP[train_size:]\n",
        "  #MSE\n",
        "  MSE = ((DP-GT)**2).sum()/len(DP)\n",
        "  print(\"MSE : {}\\n\".format(MSE))\n",
        "  #MAE\n",
        "  MAE = (np.abs(DP-GT)).sum()/len(DP)\n",
        "  print(\"MAE : {}\\n\".format(MAE))\n",
        "  #MAPE\n",
        "  MAPE = (np.abs(1-DP/GT)).sum()/len(DP)\n",
        "  print(\"MAPE : {}\\n\".format(MAPE))\n",
        "  #RMSE\n",
        "  RMSE = math.sqrt(((DP-GT)**2).sum()/len(DP))\n",
        "  print(\"RMSE : {}\".format(RMSE))\n",
        "  return (MSE, MAE, MAPE, RMSE)\n",
        "\n",
        "\n",
        "def int2act_func(num):\n",
        "  if num == 0:\n",
        "    activation_function = nn.Tanh()\n",
        "  elif num == 1:\n",
        "    activation_function = nn.Sigmoid()\n",
        "  elif num == 2:\n",
        "    activation_function = nn.ReLU()\n",
        "  elif num == 3:\n",
        "    activation_function = nn.Softsign()\n",
        "  elif num == 4:\n",
        "    activation_function = nn.ELU()\n",
        "  return activation_function\n",
        "\n",
        "def num2act(num):\n",
        "  if num == 0:\n",
        "    act = \"Tanh\"\n",
        "  elif num == 1:\n",
        "    act = \"Sigmoid\"\n",
        "  elif num == 2:\n",
        "    act = \"ReLU\"\n",
        "  elif num == 3:\n",
        "    act = \"Softsign\"\n",
        "  elif num == 4:\n",
        "    act = \"ELU\"\n",
        "  return act\n",
        "\n",
        "\n",
        "def int2optim_func(num, net, lr):\n",
        "  if num == 0:\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "  elif num == 1:\n",
        "    optimizer = torch.optim.AdamW(net.parameters(), lr=lr)\n",
        "  elif num == 2:\n",
        "    optimizer = torch.optim.Adagrad(net.parameters(), lr=lr)\n",
        "  elif num == 3:\n",
        "    optimizer = torch.optim.ASGD(net.parameters(), lr=lr)\n",
        "  return optimizer\n",
        "\n",
        "def num2optim(num):\n",
        "  if num == 0:\n",
        "    optim = \"Adam\"\n",
        "  elif num == 1:\n",
        "    optim = \"AdamW\"\n",
        "  elif num == 2:\n",
        "    optim = \"Adagrad\"\n",
        "  elif num == 3:\n",
        "    optim = \"ASGD\"\n",
        "  return optim\n",
        "\n",
        "class CustomDataset(Dataset): \n",
        "  def __init__(self, input, label):\n",
        "    self.x_data = input\n",
        "    self.y_data = label\n",
        "\n",
        "  def __len__(self): \n",
        "    return len(self.x_data)\n",
        "\n",
        "  def __getitem__(self, idx): \n",
        "    x = torch.FloatTensor(self.x_data[idx])\n",
        "    y = torch.FloatTensor(self.y_data[idx])\n",
        "    return x, y\n",
        "\n",
        "def evaluate(\n",
        "    net: nn.Module, data_loader: DataLoader, dtype: torch.dtype, device: torch.device\n",
        ") -> float:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            total += 1\n",
        "            inputs = inputs.to(dtype=dtype, device=device)\n",
        "            labels = labels.to(device=device)\n",
        "            outputs = net(inputs)\n",
        "            correct += (outputs - labels)**2\n",
        "\n",
        "    print(\"MSE eval : {}\\ntest total : {}\".format((correct/total).item(), total))\n",
        "\n",
        "    return (correct/total).item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZAieUU6APSO"
      },
      "source": [
        "def net_train(net, train_loader, parameters, dtype, device):\n",
        "  start_time = time.time()\n",
        "\n",
        "  net.to(dtype=dtype, device=device)\n",
        "\n",
        "  lr = parameters.get(\"lr\")\n",
        "  num_epochs = parameters.get(\"num_epochs\") * 100\n",
        "  optim_num = parameters.get(\"optim_num\")\n",
        "\n",
        "  print(\"num_epoch : {}\\n\".format(num_epochs))\n",
        "  print(\"lr : {}\\n\".format(lr))\n",
        "  print(\"optim_func : {}\\n\".format(num2optim(optim_num)))\n",
        "\n",
        "  # Define loss, optimizer and dataloader\n",
        "  criterion = torch.nn.MSELoss()\n",
        "  optimizer = int2optim_func(optim_num, net, lr)\n",
        "  (x, y) = dataloader2tensor(train_loader)\n",
        "\n",
        "  print(\"trian x : {} , y : {}\".format(x.shape,y.shape))\n",
        "\n",
        "  # Train Network\n",
        "  for epoch in range(num_epochs):\n",
        "    outputs = net.forward(x.to(device))\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # obtain the loss function\n",
        "    loss = criterion(outputs, y.to(device))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  print(\"total epoch time : {}\".format(time.time()-start_time))\n",
        "\n",
        "  return net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ouIaFBeKDlo"
      },
      "source": [
        "class LSTM_Time_Series(nn.Module):\n",
        "  def __init__(self, parameters):\n",
        "    super(LSTM_Time_Series, self).__init__()\n",
        "    self.num_classes = parameters.get(\"num_classes\")\n",
        "    self.num_layers = parameters.get(\"num_layers\")\n",
        "    self.input_size = parameters.get(\"input_size\")\n",
        "    self.hidden_size = parameters.get(\"hidden_size\")\n",
        "\n",
        "    num_classes = parameters.get(\"num_classes\")\n",
        "    num_layers = parameters.get(\"num_layers\")\n",
        "    input_size = parameters.get(\"input_size\")\n",
        "    hidden_size = parameters.get(\"hidden_size\")\n",
        "    act_num = parameters.get(\"act_num\")\n",
        "\n",
        "    self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size,\n",
        "                      num_layers = num_layers, batch_first=True)\n",
        "    self.fc_1 =  nn.Linear(hidden_size, 128)\n",
        "    self.fc = nn.Linear(128, num_classes)\n",
        "    self.act = int2act_func(act_num)\n",
        "\n",
        "    print(\"hidden_size : {}\".format(hidden_size))\n",
        "    print(\"activation_function : {}\\n\".format(num2act(act_num)))\n",
        "\n",
        "  def forward(self,x):\n",
        "    h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device) #hidden state\n",
        "    c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device) #internal state   \n",
        "\n",
        "    # Propagate input through LSTM\n",
        "    output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
        "    hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
        "\n",
        "    out = self.act(hn)\n",
        "    out = self.fc_1(out)\n",
        "    out = self.act(out)\n",
        "    out = self.fc(out)\n",
        "    return out "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KQ-eMuoAoq1"
      },
      "source": [
        "def train_evaluate(parameterization):\n",
        "\n",
        "    # Get neural net\n",
        "    lstm = LSTM_Time_Series(parameterization).to(device)\n",
        "    \n",
        "    # train\n",
        "    trained_net = net_train(net=lstm, train_loader=train_loader, parameters=parameterization, dtype=dtype, device=device)\n",
        "    \n",
        "    # return the accuracy of the model as it was trained in this run\n",
        "    return evaluate(\n",
        "        net=trained_net,\n",
        "        data_loader=test_loader,\n",
        "        dtype=dtype,\n",
        "        device=device,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLyiiJ01q0Wr"
      },
      "source": [
        "def hyperparam_optim():\n",
        "\n",
        "  all_time = time.time()\n",
        "  best_parameters, values, experiment, model = optimize(\n",
        "      parameters=[\n",
        "          {\"name\": \"lr\", \"type\": \"range\", \"bounds\": [1e-7, 1e-3], \"log_scale\": True},\n",
        "          {\"name\": \"num_epochs\", \"type\": \"range\", \"bounds\": [10, 100]},\n",
        "          {\"name\": \"hidden_size\", \"type\": \"range\", \"bounds\": [1, 500]},\n",
        "          {\"name\": \"optim_num\", \"type\": \"choice\", \"values\": [0,1,2,3]},\n",
        "          {\"name\": \"act_num\", \"type\": \"choice\", \"values\": [0,1,2,3,4]},\n",
        "          {\"name\": \"input_size\", \"type\": \"fixed\", \"value\": 403},\n",
        "          {\"name\": \"num_layers\", \"type\": \"fixed\", \"value\": 1},\n",
        "          {\"name\": \"num_classes\", \"type\": \"fixed\", \"value\": 1},\n",
        "      ],\n",
        "      minimize = True,\n",
        "      total_trials = 500, # total iteration for hyperparameter optimization\n",
        "      evaluation_function=train_evaluate,\n",
        "      objective_name='MSE',\n",
        "  )\n",
        "  print(\"total time : {}\".format(time.time()-all_time))\n",
        "  print(\"\\n=======================\\nbest_parameters\\nlr : {}\\nnum_epochs : {}\\nhidden_size : {}\\noptimization : {}\\nactivation : {}\\n\".format(\n",
        "      best_parameters[\"lr\"],best_parameters[\"num_epochs\"],best_parameters[\"hidden_size\"],num2optim(best_parameters[\"optim_num\"]),num2act(best_parameters[\"act_num\"])))\n",
        "  means, covariances = values\n",
        "  print(means)\n",
        "  print(covariances)\n",
        "\n",
        "  best_objectives = np.array([[trial.objective_mean*100 for trial in experiment.trials.values()]])\n",
        "\n",
        "  best_objective_plot = optimization_trace_single_method(\n",
        "      y=np.minimum.accumulate(best_objectives, axis=1),\n",
        "      title=\"Model performance vs. # of iterations\",\n",
        "      ylabel=\"MSE\",\n",
        "  )\n",
        "  render(best_objective_plot)\n",
        "\n",
        "  return (best_parameters, values, experiment, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2bIRLaWKpkX"
      },
      "source": [
        "dtype = torch.float\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "garch = date2index(garch)\n",
        "egarch = date2index(egarch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igvlEKdxAVbs"
      },
      "source": [
        "garch = garch.to_numpy()\n",
        "egarch = egarch.to_numpy()\n",
        "VIX = VIX.iloc[:, 1:2].to_numpy()\n",
        "\n",
        "train_size = 3465"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GD5GrIYq3TL"
      },
      "source": [
        "# hyperparameter optimization for garch data\n",
        "(X_train, y_train, X_test, y_test)= make_train_test(garch, VIX, train_size)\n",
        "\n",
        "train_dataset = CustomDataset(X_train,y_train)\n",
        "train_loader = DataLoader(train_dataset)\n",
        "test_dataset = CustomDataset(X_test,y_test)\n",
        "test_loader = DataLoader(test_dataset)\n",
        "\n",
        "(best_parameters_g, values_g, experiment_g, model_g) = hyperparam_optim()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8u5Aa06eLNFV"
      },
      "source": [
        "# grach training\n",
        "lr = best_parameters_g[\"lr\"]\n",
        "num_epochs = best_parameters_g[\"num_epochs\"] * 100\n",
        "optim_num = best_parameters_g[\"optim_num\"]\n",
        "\n",
        "(X_train_tensors_final_g,X_test_tensors_final_g,y_train_tensors_g,y_test_tensors_g) = make_train_test_tensor(garch, VIX, train_size)\n",
        "\n",
        "lstm_g = LSTM_Time_Series(best_parameters_g).to(device)\n",
        "\n",
        "loss_function_g = torch.nn.MSELoss()    # mean-squared error for regression\n",
        "optimizer_g = int2optim_func(optim_num, lstm_g, lr)\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  outputs_g = lstm_g.forward(X_train_tensors_final_g.to(device)) #forward pass\n",
        "  optimizer_g.zero_grad() #caluclate the gradient, manually setting to 0\n",
        "\n",
        "  # obtain the loss function\n",
        "  loss_g = loss_function_g(outputs_g, y_train_tensors_g.to(device))\n",
        "\n",
        "  loss_g.backward() #calculates the loss of the loss function\n",
        "\n",
        "  optimizer_g.step() #improve from loss, i.e backprop\n",
        "\n",
        "print(\"total time : {}\".format(time.time()-start_time))\n",
        "\n",
        "# garch errors\n",
        "(GT_g, DP_g) = visualize(garch, VIX, lstm_g, train_size)\n",
        "(MSE_g, MAE_g, MAPE_g, RMSE_g) = errors(GT_g, DP_g, train_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOpCt8IwrBIz"
      },
      "source": [
        "# hyperparameter optimization for egarch data\n",
        "(X_train, y_train, X_test, y_test)= make_train_test(egarch, VIX, train_size)\n",
        "\n",
        "train_dataset = CustomDataset(X_train,y_train)\n",
        "train_loader = DataLoader(train_dataset)\n",
        "test_dataset = CustomDataset(X_test,y_test)\n",
        "test_loader = DataLoader(test_dataset)\n",
        "\n",
        "(best_parameters_eg, values_eg, experiment_eg, model_eg) = hyperparam_optim()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhuFtYD8umr7"
      },
      "source": [
        "# egrach training\n",
        "lr = best_parameters_eg[\"lr\"]\n",
        "num_epochs = best_parameters_eg[\"num_epochs\"] * 100\n",
        "optim_num = best_parameters_eg[\"optim_num\"]\n",
        "\n",
        "(X_train_tensors_final_eg,X_test_tensors_final_eg,y_train_tensors_eg,y_test_tensors_eg) = make_train_test_tensor(egarch, VIX, train_size)\n",
        "\n",
        "lstm_eg = LSTM_Time_Series(best_parameters_eg).to(device)\n",
        "\n",
        "loss_function_eg = torch.nn.MSELoss()    # mean-squared error for regression\n",
        "optimizer_eg = int2optim_func(optim_num, lstm_eg, lr)\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  outputs_eg = lstm_eg.forward(X_train_tensors_final_eg.to(device)) #forward pass\n",
        "  optimizer_eg.zero_grad() #caluclate the gradient, manually setting to 0\n",
        "\n",
        "  # obtain the loss function\n",
        "  loss_eg = loss_function_eg(outputs_eg, y_train_tensors_eg.to(device))\n",
        "\n",
        "  loss_eg.backward() #calculates the loss of the loss function\n",
        "\n",
        "  optimizer_eg.step() #improve from loss, i.e backprop\n",
        "\n",
        "print(\"total time : {}\".format(time.time()-start_time))\n",
        "\n",
        "# egarch errors\n",
        "(GT_eg, DP_eg) = visualize(garch, VIX, lstm_eg, train_size)\n",
        "(MSE_eg, MAE_eg, MAPE_eg, RMSE_eg) = errors(GT_eg, DP_eg, train_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NG-2wpR9JQ2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTrxXdoSIi-Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}